# 深度学习大作业: 拼图问题

### 摘要

本次作业中, 针对拼图问题, 我们基于Jittor框架, 在CIFAR10数据集上复现了前人的DeepPermNet工作, 并且对比了Sinkhorn方法使用与否以及不同的损失函数对模型表现的影响. 最后, 我们提出了具有潜力的**相关法**解决问题, 达到了**90.0%**的测试集精度, **高于基线3.5%**.

### 问题背景&相关工作

**拼图问题**: 给定一张图, 将其横竖切为4等分的子图片, 将这4张子图的顺序打乱. 要求根据这些乱序的子图, 还原出它们原先的排序.

**排列**: 要表示一个$n$元的共计$n!$种的排列, 可使用$n\cross n$维排列矩阵表示排列前后每个元素位置的对应关系. 例如$\begin{equation*} \begin{pmatrix} 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0  \\ 0 & 0 & 1 & 0 \\0 & 0 & 0 & 1  \end{pmatrix} \end{equation*}$ 对应于$[1,0,2,3]$. 要求排列矩阵的元素在$\{0,1\}$中取值, 矩阵的各行, 各列有且仅有一个元素为1.

**DeepPermNet**$^{[1]}$: 为了解决上述问题, 前人提出 DeepPermNet 的方法. 将4份子图分别经过卷积神经网络得到图像表征, 将4份图像表征拼接后经过全连接层, 得到4x4维的输出. 使用 Sinkhorn 算法$^{[2]}$将输出转化成双随机矩阵, 寻找距离双随机矩阵最近的排列矩阵作为算法预测.

<img src="assets/图片1.png" alt="图片1" style="zoom: 33%;" />

**Sinkhorn&双随机矩阵**: Sinkhorn 算法使用可微分运算将任意方阵转化为双随机矩阵. 双随机矩阵是指各行,各列之和均为1的非负方阵. 所有双随机矩阵构成 Birkhoff 多面体, 而所有排列矩阵是 Birkhoff 多面体的全部顶点. 可以证明双随机矩阵是排列矩阵的凸组合, 因而双随机矩阵可以认为是排列矩阵的连续形式.

### 初步工作

我们首先基于Jittor框架, 复现**DeepPermNet**的工作, 在CIFAR10数据集上进行训练并调整模型. 

CIFAR10数据集由50000张训练集图像,10000张测试集图像构成, 每张图像的尺寸为 $3\cross32\cross 32$ . 考虑到切分图像后每张子图的尺寸较小, 我们基于 AlexNet 网络, 调整模型参数, 得到所使用的网络. 同时, 我们额外从训练集中划分5%的数据作为验证集.

由于原始算法使用的Sinkhorn算法和依据Frobenius范数重建排列矩阵的方法有较大的计算成本, 我们在训练时首先删去这两个步骤. 使用模型的4x4输出直接作为排列矩阵的连续近似, 取矩阵每行的最大元素序号作为离散化的预测结果.

另外, 注意到Jittor框架提供的`jittor.nn.CrossEntropyLoss()`方法在计算交叉熵损失之前, 先对所有输入进行了Softmax运算. 尽管直接使用这个方法不符合我们的预期, 由于它仍能正常训练, 我们仍然使用这个方法, 并留待后续进行对比实验. 计算交叉熵时, 我们将模型输出的$4\cross4$维的矩阵与4维的排列向量输入损失函数. jittor的交叉熵函数会对输入矩阵的每行作softmax, 这意味着4维的排列向量被当成了4个独立的分类标签进行预测.

最终, 我们达到了验证集准确率0.873866, 测试集准确率0.865108.

我们使用的模型架构, 训练参数配置以及训练曲线如下: 

<img src="assets/12.png" alt="12" style="zoom:40%;" />
<center>
<img src="assets/8.png" alt="8" style="zoom: 25%;" />
<img src="assets/5_m.png" alt="5_m" style="zoom: 25%;" />
</center>

我们将这个结果作为任务的基线. 为了对比, 在后续的实验探究中不再更改模型的整体架构.

### Loss函数&Sinkhorn

先前提到, 基线模型的训练中没有处理Jittor提供的交叉熵函数不符合预期的问题, 没有使用sinkhorn算法. 

在这个部分中, 我们希望按照预期功能重新实现交叉熵函数, 并且探究不同的损失函数以及是否使用Sinkhorn算法对最终表现的影响. 如前所述, 我们不更改先前模型的整体架构, 仅修改训练参数配置.

我们将jittor提供的交叉熵函数, 重新实现的交叉熵函数, MSE损失函数分别称为`jittor_cel`,`my_cel`,`my_mse` 以进行实验对比.

<img src="assets/11.png" alt="11" style="zoom: 35%;" />

`my_cel+sinkhorn`方法无法完成训练. 我们观察进行了少量训练后的模型后发现, 这个方法倾向于让神经网络的输出的部分项趋于无穷, 部分项趋于0. 这时可能由于数值计算的精度问题或算法本身的收敛性问题, sinkhorn算法的输出结果并非双随机矩阵.

`my_mse`和`my_mse+sinkhorn`方法能够完成训练, 但不能完成任务. 观察模型输出发现, 这两个方法倾向于让模型输出各元素均为$\frac{1}{4}$的方阵. 这使得模型不能积极地往最优点训练, 因而不能完成任务.

因此, 基线中使用的`jittor_cel`方法仍然是最为合适的方法.

### 相关法

这一部分中, 我们提出了相关法取代sinkhorn算法和它随后的交叉熵损失, 并通过实验展现了它的优秀表现.

#### Sinkhorn算法的问题分析

尽管Sinkhorn算法能将模型输出转变为双随机矩阵, 它仍然存在诸多问题. 

* Sinkhorn的输入与输出没有理想的对应关系. 例如:

<img src="assets/6.png" alt="6" style="zoom: 33%;" />

<img src="assets/4.png" alt="4" style="zoom: 33%;" />

​	第一个示例中,$[3,3]$位置的元素9是所在行列的最大值, 但在运算后, $[3,3]$位置却并非该行该列后的最大值. 第二个示例中, 输入矩阵是逐行逐列递增的, 但输出矩阵是各元素均为$\frac{1}{4}$的方阵. 

* Sinkhorn的计算耗时较大. 相比于不使用Sinkhorn的情形, 模型的训练时间要高出5~10倍.
* Sinkhorn+排列矩阵近似的方法缺乏严密的数学逻辑, 输出结果缺乏可解释性. 这可能给神经网络的训练带来误导.

这些问题说明, Sinkhorn算法可能并不适合作为模型输出后的处理环节. 因此我们试图重新建模原问题, 绕过Sinkhorn算法.

拼图问题需要从 $n!$ 种排列的均匀分布中预测正确的一项, 形式上说这是有$n!$类的多分类预测问题. 但直接依照这个思路处理的问题是它将$n!$类视为相互独立的分类, 而破坏了排列空间的结构, 训练模型时无法捕捉不同种错误预测与正确标签的不同相关性. 

而DeepPermNet的思路是将排列空间放回 Birkhoff 多面体, 以便通过双随机矩阵的形式更好的捕捉空间结构. Birkhoff 多面体是$n^2$维空间中的$(n-1)^2$维流形. 这使得我们不得不压缩模型输出空间至Birkhoff多面体, 并在预测是近似为离散化的排列矩阵. 这个过程借助了Sinkhorn算法和离散化近似, 因而引入了前述问题.

#### 算法描述

我们希望绕过Birkhoff多面体和双随机矩阵这两个工具, **直接解决多分类问题, 同时保留排列空间的结构**. 因此提出了**相关法**.

考虑到排列空间是由$n\cross n$维空间中的$n!$个元素构成, 我们将这个$n\cross n$维空间拍平成为$n^2$维欧式空间, 并计算模型输出$x\in \mathbb R^{n^2}$与$n!$个排列元素的内积, **将这$n!$个内积值作为不同种排列的预测分数**, 然后使用Softmax和交叉熵损失函数进行模型训练.

作内积的原因是它可以表征高维向量的相似程度. 也可以考虑采用其他表示相似程度的度量, 如高斯核函数, 多项式核函数.

通过这个方法, 我们达到了验证集准确率0.912897, 测试集准确率0.900316. 训练曲线如下:

<img src="assets/10_m.png" alt="10_m" style="zoom: 33%;" />

可以看到, 相关法: (1) 表现超出了基线. (2) 训练速度快于使用Sinkhorn算法的情形. (3) 有一定的可解释性. 

因而这个方法有吸引人的潜力. 但它的主要问题在于扩展性弱. 当排列的位数增大时, 排列空间的大小按$n!$增长, 计算代价无法承受. 通过交叉熵损失函数的定义可见, 要想降低训练开销, 可以仅仅计算标签对应的排列元素与模型输出的内积. 但准确的推理仍要计算所有排列. 可以采用近似方法降低推理开销, 比如按行取最大元素对应位置作为预测.

### 总结

为了解决拼图问题, 我们复现了DeepPermNet, 探究了Sinkhorn操作和不同Loss选取对模型表现的影响, 并提出了有潜力的相关法来解决问题, 取得了优秀的表现.

### 致谢&参考文献

参考文献:

* [1]: [[1704.02729\] DeepPermNet: Visual Permutation Learning (arxiv.org)](https://arxiv.org/abs/1704.02729)
* [2]: R. Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876–879, 1964.  

致谢:

* 感谢张睿, 李天骅, 毛宇琛等同学与我共同讨论.